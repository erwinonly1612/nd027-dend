{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Movie Recommmendation with Spark and AWS\n",
    "## Introduction\n",
    "\n",
    "The project uses datasets (ml-latest-small) from [MovieLens](https://grouplens.org/datasets/movielens/latest/), a movie recommendation service. It contains 100836 ratings and 3683 tags across 9742 movies. The ratings were created by 610 users between 1996 and 2018. The larger dataset contains 27753444 ratings and 1108997 tags across 58098 movies. Ratings were created by 283228 users between 1995 and 2018.\n",
    "\n",
    "I also generated two txt files for movies with awards. The file is copied from [Wikipedia/Award-winning films](https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films)\n",
    "\n",
    "The Project is to build an ETL pipeline that extracts data from S3, processes them using Spark, stages them in Redshift, and transforms data into a set of dimensional tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col, isnan, when, count, trim, desc, sum, asc\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import countDistinct, explode, split, concat_ws, collect_list\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 1: Get the params of the created redshift cluster \n",
    "- This is for reading data from S3 to redshift\n",
    "- We need:\n",
    "    - The redshift cluster <font color='red'>endpoint</font>\n",
    "    - The <font color='red'>IAM role ARN</font> that give access to Redshift to read from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['KEY']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# e.g. DWH_ENDPOINT=\"redshift-cluster-1.csmamz5zxmle.us-west-2.redshift.amazonaws.com\" \n",
    "DWH_ENDPOINT=\"\" \n",
    "    \n",
    "#e.g DWH_ROLE_ARN=\"arn:aws:iam::988332130976:role/dwhRole\"\n",
    "DWH_ROLE_ARN=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 2: Explore and Assess the Data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Part 1: Load Data from S3 and clean dataframe\n",
    "- movie.csv: including movieId, title(year), genres\n",
    "  - split title and year from the second column\n",
    "  - split generes from the array\n",
    "- ratings.csv: including userId, movieId, rating, ts\n",
    "  - transform ts string into timestamp\n",
    "- tags.csv: including userId, movieId, tag, ts\n",
    "  - transform ts string into timestamp\n",
    "- awards.txt: including Film, year, awards, nominations\n",
    "  - split txt data using delimiter \"|\"\n",
    "  - identify issues when splitting data like inappropriate year\n",
    "  - transform data into appropriate data type\n",
    "- award_corrected.txt: including Film, year, awards, nominations (corrections for awards.txt)\n",
    "  - join with awards to correct the year\n",
    "  - transform data into appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movieSchema = R([\n",
    "            Fld(\"movieId\",Int()),\n",
    "            Fld(\"title\",Str()),\n",
    "            Fld(\"genres\",Str())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ratingSchema = R([\n",
    "            Fld(\"userId\",Int()),\n",
    "            Fld(\"movieId\",Int()),\n",
    "            Fld(\"rating\",Dbl()),\n",
    "            Fld(\"ts\",Str())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tagSchema = R([\n",
    "            Fld(\"userId\",Int()),\n",
    "            Fld(\"movieId\",Int()),\n",
    "            Fld(\"tag\",Str()),\n",
    "            Fld(\"ts\",Str())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read movies, ratings, and tags csv\n",
    "dfmovies = spark.read.csv(\"s3a://udacity-input/ml-latest-small/movies.csv\", header=True, schema=movieSchema)\n",
    "dfratings = spark.read.csv(\"s3a://udacity-input/ml-latest-small/ratings.csv\", header = True, schema=ratingSchema)\n",
    "dftags = spark.read.csv(\"s3a://udacity-input/ml-latest-small/tags.csv\", header = True, schema=tagSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read awards txt\n",
    "dfawards = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"s3a://udacity-input/ml-latest-small/Awards.txt\")\n",
    "\n",
    "dfawards.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read award_corrected txt\n",
    "dfawards2 = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"s3a://udacity-input/ml-latest-small/Award_corrected.txt\")\n",
    "\n",
    "dfawards2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 3: Define Relational Data Model\n",
    "**For the following use cases, I created 5 tables**\n",
    "- number of movies in the dataset  \n",
    "- number of movies in each genre  \n",
    "- number of users in the dataset  \n",
    "- Minimum number of ratings per user  \n",
    "- Minimum number of ratings per movie   \n",
    "- number of movies not rated  \n",
    "- the top 5 movies with high ratings  \n",
    "- number of movies receiving awards  \n",
    "- total awards that movie received  \n",
    "- number of movies rated and receiving awards  \n",
    "- the average rating scores of movies with awards  \n",
    "- year durations in movies, ratings and awards dataset  \n",
    "\n",
    "**snowflake schema**\n",
    "* **awards** - (film, year, nominations, awards)  \n",
    "This table will have the awards that each movie received. The composite key of film and year is used to identify each row in this table since films can be made in the same name. \n",
    "* **movies** - (movieId, title, year)  \n",
    "The primary key for movies is movieId, and genres need to removed from the original table since genres include a list of genres for each movie.\n",
    "* **genres** - (genreId, movieId, genre)  \n",
    "A separate table genres needs to be created to identify the type of each movie. Since each movie can have several types, a unique id genreId is created for this table as primary key.  \n",
    "* **ratings** - (userId, movieId, rating, rate_time, year)  \n",
    "The composite key is userId and movieId in ratings table since a user can rate different movies.\n",
    "* **time** - timestamps in ratings broken down into specific units (date_key, day, week, month, year)\n",
    "A time table is created to check the day, week, month and year. The primary key is date_key.\n",
    "\n",
    "#### Method 1: Mapping Out Data Pipelines using Spark\n",
    "- Movies and genres can be created using the movies csv from S3.\n",
    "- Ratings can be created using the ratings csv from S3.\n",
    "- Awards can be created by joining data in awards.txt and award_correction.txt.\n",
    "\n",
    "#### Method 2: Mapping Out Data Pipelines in Redshift\n",
    "- Awards, ratings, genres table in parquet format can be read directly from S3.  \n",
    "- Movies and genres can be created using the movies data from S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model using Spark\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfmovies.printSchema()\n",
    "dfmovies.show(5, truncate = False)\n",
    "dfmovies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert timestamp\n",
    "dfratings = dfratings.withColumn(\n",
    "    \"rate_time\",\n",
    "    F.to_timestamp(F.from_unixtime((col(\"ts\")) , 'yyyy-MM-dd HH:mm:ss.SSS')).cast(\"Timestamp\")\n",
    ").drop(\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings = dfratings.withColumn(\"year\", F.year(\"rate_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings.printSchema()\n",
    "dfratings.show(5)\n",
    "dfratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert timestamp\n",
    "dftags = dftags.withColumn(\"tag_time\", F.to_timestamp(col(\"ts\") / 1)).drop(\"ts\")\n",
    "dftags = dftags.withColumn(\"year\", F.year(\"tag_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dftags.printSchema()\n",
    "dftags.show(5)\n",
    "dftags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean awards txt flie\n",
    "dfawards = dfawards.withColumn(\"film\", dfawards['Film   '].cast(Str())).drop('Film   ')\n",
    "dfawards = dfawards.withColumn(\"year\", dfawards['Year   '].cast(Int())).drop(\"Year   \")\n",
    "dfawards = dfawards.withColumn(\"awards\", dfawards['Awards    '].cast(Dbl())).drop(\"Awards    \")\n",
    "dfawards = dfawards.withColumn(\"nominations\", dfawards['Nominations'].cast(Int()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards2 = dfawards2.withColumn(\"film\", dfawards2['Film   '].cast(Str())).drop('Film   ')\n",
    "dfawards2 = dfawards2.withColumn(\"year\", dfawards2['Year   '].cast(Int())).drop(\"Year   \")\n",
    "#dfawards2 = dfawards2.withColumn(\"date\", F.to_timestamp(col('Year   '))).drop('Year   ')\n",
    "#dfawards2 = dfawards2.withColumn(\"year\", F.year(\"date\")).drop(\"date\")\n",
    "dfawards2 = dfawards2.withColumn(\"awards\", dfawards2['Awards    '].cast(Dbl())).drop(\"Awards    \")\n",
    "dfawards2 = dfawards2.withColumn(\"nominations\", dfawards2['Nominations'].cast(Int()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards.printSchema()\n",
    "dfawards.show(5, truncate = False)\n",
    "dfawards.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards2.printSchema()\n",
    "dfawards2.show(5, truncate = False)\n",
    "dfawards2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split the mixed genres by '|'\n",
    "dfmovies2 = dfmovies.withColumn('genre', explode(split(dfmovies.genres, '\\|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfmovies2.show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create genere information for each movie\n",
    "dfgenre = dfmovies2.select(\"movieId\", \"genre\").dropDuplicates().dropna(subset=[\"movieId\", \"genre\"]).withColumn(\"genreId\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dfgenre.filter(dfgenre.title.contains('Toy Story (1995)')).show()\n",
    "dfgenre.filter(dfgenre.movieId == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfgenre.columns\n",
    "dfgenre.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load Data to S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards.write.parquet(\"s3a://sparkifydend/movies/awards/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards2.write.parquet(\"s3a://sparkifydend/movies/awards2/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfmovies.write.parquet(\"s3a://sparkifydend/movies/movies/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings.write.parquet(\"s3a://sparkifydend/movies/ratings/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dftags.write.parquet(\"s3a://sparkifydend/movies/tags/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfgenre.write.parquet(\"s3a://sparkifydend/movies/genres/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards = spark.read.parquet(\"s3a://sparkifydend/movies/awards/*\")\n",
    "dfawards2 = spark.read.parquet(\"s3a://sparkifydend/movies/awards2/*\")\n",
    "dfmovies = spark.read.parquet(\"s3a://sparkifydend/movies/movies/*\")\n",
    "dfratings = spark.read.parquet(\"s3a://sparkifydend/movies/ratings/*\")\n",
    "dftags = spark.read.parquet(\"s3a://sparkifydend/movies/tags/*\")\n",
    "dfgenre = spark.read.parquet(\"s3a://sparkifydend/movies/genres/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks Part 1: Identify missing values, duplicate data, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "dfmovies.select([count(when(col(c).isNull(), c)).alias(c) for c in dfmovies.columns]).show()\n",
    "dfratings.select([count(when(col(c).isNull(), c)).alias(c) for c in dfratings.columns]).show()\n",
    "dfawards.select([count(when(col(c).isNull(), c)).alias(c) for c in dfawards.columns]).show()\n",
    "dfawards2.select([count(when(col(c).isNull(), c)).alias(c) for c in dfawards2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show records with year < 1920\n",
    "dfawards.filter(dfawards.year < 1920).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check records in dfawards2\n",
    "dfawards2.filter(trim(dfawards2.film) == \"Joker\").show()\n",
    "dfawards2.filter(trim(dfawards2.film) == \"Once Upon a Time in Hollywood\").show()\n",
    "dfawards2.filter(trim(dfawards2.film) == \"1917\").show()\n",
    "dfawards2.filter(trim(dfawards2.film) == \"Roma\").show()\n",
    "dfawards2.filter(trim(dfawards2.film) == \"The Favourite\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop records with wrong year \n",
    "dfawards = dfawards.filter(dfawards.year > 1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards.select([count(when(col(c).isNull(), c)).alias(c) for c in dfawards.columns]).show()\n",
    "dfawards.show(5, truncate = False)\n",
    "dfawards.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# union dfawards and dfawards2, and remove duplicates\n",
    "# dfawards2 has corrections for year\n",
    "dfawards3 = dfawards.union(dfawards2).distinct().filter(~col(\"year\").isin([0]) & col(\"year\").isNotNull()).sort(desc('year'))\n",
    "dfawards3.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show records with year not in the right range\n",
    "dfawards3.where(dfawards3.year < 1920).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load to S3\n",
    "dfawards3.write.parquet(\"s3a://sparkifydend/movies/awards3/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks Part 2: source/count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_check(df, tablename):\n",
    "    '''\n",
    "    Input: Spark dataframe, table name\n",
    "    Output: Print outcome of data quality check\n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(tablename))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(tablename, result))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform data quality check with unit test\n",
    "quality_check(dfmovies, \"movies table\")\n",
    "quality_check(dfratings, \"ratings table\")\n",
    "quality_check(dfawards3, \"awards table\")\n",
    "quality_check(dfgenre, \"genre table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfmovies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfmovies[['movieId']].drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dfratings is on movieid and userid level\n",
    "dfratings[['movieId', 'userId']].drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dfawards3 is on title and year level\n",
    "dfawards3[['film', 'year']].drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check out movies with same name\n",
    "df1 = dfawards3.groupBy(\"film\").count().filter(\"count > 1\")\n",
    "df1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards3.filter(trim(dfawards3.film) == \"A Star Is Born\").show()\n",
    "dfawards3.filter(trim(dfawards3.film) == \"Titanic\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data Wrangling with Spark and OLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use the dataframe dfmovies2 to match every movie to a single genre\n",
    "genre_movies = dfmovies2 \\\n",
    "                    .groupBy(dfmovies2.genre) \\\n",
    "                    .agg(concat_ws(',', collect_list(dfmovies2.movieId)) \\\n",
    "                    .alias('MovieIds')) \\\n",
    "                    .orderBy('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "genre_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use case\n",
    "# number of movies in the dataset\n",
    "distinct_movie = dfmovies.select(\"movieId\").distinct().count()\n",
    "print('{} movies in the movies dataset'.format(distinct_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of users in the dataset\n",
    "distinct_user = dfratings.select(\"userId\").distinct().count()\n",
    "print('{} users rated the movies'.format(distinct_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of movies receiving awards\n",
    "distinct_award = dfawards3.select(\"film\", \"year\").distinct().count()\n",
    "print('{} movies received awards'.format(distinct_award))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# show movies receiving more than 10 awards\n",
    "dfawards3.where(dfawards3.awards > 10).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# total awards that movie received\n",
    "awards_cnt = dfawards3.groupBy(\"film\", \"year\").agg(F.sum(\"awards\").alias('cnt')).orderBy(desc('cnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "awards_cnt.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Minimum number of ratings per user\n",
    "# Minimum number of ratings per movie \n",
    "tmp1 = dfratings.groupBy(\"userID\").count().toPandas()['count'].min()\n",
    "tmp2 = dfratings.groupBy(\"movieId\").count().toPandas()['count'].min()\n",
    "print('For the users that rated movies and the movies that were rated:')\n",
    "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
    "print('Minimum number of ratings per movie is {}'.format(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count number of movies in each genre\n",
    "# The top three genres are drama, comedy, and thriller\n",
    "df2=dfmovies2.groupBy(\"genre\").count().filter(trim(dfmovies2.genre) != '(no genres listed)').sort(desc('count'))\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings.createOrReplaceTempView(\"ratings\")     #userId, movieId, rating, rate_time, year\n",
    "dfmovies.createOrReplaceTempView(\"movies\")       #movieId, title, genre\n",
    "dftags.createOrReplaceTempView(\"tags\")           #userId, movieId, tag, tag_time, year\n",
    "dfawards3.createOrReplaceTempView(\"awards\")      #nominations, film, year, awards\n",
    "dfgenre.createOrReplaceTempView(\"genres\")        #genreId, genre, movieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split title and release year in separate columns     \n",
    "movies = spark.sql(\"select movieId, substr(title, 0, length(title)-7) as title, substr(title, -5, 4) as year from movies\")\n",
    "movies.show()\n",
    "movies.createOrReplaceTempView(\"movies\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# year of movies in the dataset\n",
    "spark.sql(\"\"\"select \n",
    "             min(year) as min_year,\n",
    "             max(year) as max_year\n",
    "             from movies \n",
    "             where year > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# year of rating in the dataset\n",
    "spark.sql(\"\"\"select \n",
    "             min(year) as min_year,\n",
    "             max(year) as max_year\n",
    "             from ratings\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# year of awards in the dataset\n",
    "spark.sql(\"\"\"select \n",
    "             min(year) as min_year,\n",
    "             max(year) as max_year\n",
    "             from awards\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of movies not rated\n",
    "spark.sql(\"\"\"select \n",
    "          count(distinct movies.movieId)\n",
    "          from movies \n",
    "          where movies.movieId not in\n",
    "          (select distinct ratings.movieId from ratings)\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of movies rated and receiving awards\n",
    "# 474 movies receiving awards and shown in ratings dataset\n",
    "spark.sql(\"\"\"select count(distinct movieId) as in_ratings from \n",
    "          (select distinct a.film, a.year, m.movieId as movieId\n",
    "          from awards as a inner join movies as m on trim(a.film) == trim(m.title) and a.year = m.year\n",
    "          where a.year > 0 and m.year > 0) t\n",
    "          where movieId in \n",
    "          (select distinct ratings.movieId from ratings)\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the top 5 movies with high ratings\n",
    "avg_rating = spark.sql(\"\"\"select distinct\n",
    "    m.title as title,\n",
    "    m.year as year,\n",
    "    sum(case when r.rating >= 0 then 1 else 0 end) as num_rating,\n",
    "    avg(r.rating) as avg_rating\n",
    "    from movies as m inner join ratings as r on m.movieId = r.movieId\n",
    "    group by m.title, m.year\n",
    "    order by avg_rating desc\n",
    "\"\"\")\n",
    "avg_rating.show(5)\n",
    "avg_rating.createOrReplaceTempView(\"avg_rating\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# total awards for each movie\n",
    "tot_awards = spark.sql(\"\"\"select distinct\n",
    "                    film,\n",
    "                    year,\n",
    "                    sum(awards) as tot_awards\n",
    "                    from awards\n",
    "                    group by film, year\n",
    "                    order by tot_awards desc\n",
    "\"\"\")\n",
    "tot_awards.show(5)\n",
    "tot_awards.createOrReplaceTempView(\"tot_awards\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the average rating scores of movies with awards\n",
    "movie_awards_rating = spark.sql(\"\"\"select distinct\n",
    "             a.film,\n",
    "             a.year,\n",
    "             a.tot_awards,\n",
    "             r.avg_rating\n",
    "             from tot_awards as a inner join avg_rating as r on trim(a.film) == trim(r.title) and a.year == r.year\n",
    "             where a.year > 0 and r.year > 0\n",
    "             order by tot_awards desc, avg_rating desc\n",
    "\"\"\")\n",
    "movie_awards_rating.show(truncate = False)\n",
    "movie_awards_rating.createOrReplaceTempView(\"movie_awards_rating\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from tot_awards\").show()\n",
    "spark.sql(\"select count(*) from avg_rating\").show()\n",
    "spark.sql(\"select count(*) from movie_awards_rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.4 Create the data model using Redshift\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Extract parquet data from S3 and transform into fact and dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "s3bucket =  s3.Bucket(\"udacity-input\") # private\n",
    "\n",
    "s3_data = iter(s3bucket.objects.filter(Prefix=\"ml-latest-small/\"))\n",
    "for _ in range(5): print(next(s3_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### copy data from s3 to redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qry = \"\"\"\n",
    "    copy dimRatings from 's3://sparkifydend/movies/ratings/' \n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from dimRatings limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from dimRatings;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### check the stl_load_errors table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select query, substring(filename,22,25) as filename,line_number as line, \n",
    "substring(colname,0,12) as column, type, position as pos, substring(raw_line,0,30) as line_text,\n",
    "substring(raw_field_value,0,15) as field_text, \n",
    "substring(err_reason,0,45) as reason\n",
    "from stl_load_errors \n",
    "order by query desc\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qry = \"\"\"\n",
    "    copy dimAwards3 from 's3://sparkifydend/movies/awards3/' \n",
    "    credentials 'aws_iam_role={}' \n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from dimAwards3 limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from dimAwards3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qry = \"\"\"\n",
    "    copy dimGenres from 's3://sparkifydend/movies/genres/' \n",
    "    credentials 'aws_iam_role={}' \n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    " \n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from dimGenres limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from dimGenres;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dimDate (date_key, year, month, day, week)\n",
    "SELECT DISTINCT(rate_time)                                       AS date_key,\n",
    "       EXTRACT(year FROM rate_time)                              AS year,\n",
    "       EXTRACT(month FROM rate_time)                             AS month,\n",
    "       EXTRACT(day FROM rate_time)                               AS day,\n",
    "       EXTRACT(week FROM rate_time)                              AS week\n",
    "FROM dimRatings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from dimDate limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from dimDate;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qry = \"\"\"\n",
    "    copy dimmovies0 from 's3://sparkifydend/movies/movies/' \n",
    "    credentials 'aws_iam_role={}' \n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    " \n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dimMovies (movieId, title, year)\n",
    "SELECT movieId                                                      AS movieId,\n",
    "       substring(title, 0, length(title)-6)                         AS title, \n",
    "       substring(title, length(title)-4, 4)                         AS year\n",
    "FROM dimMovies0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from dimMovies limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from dimMovies;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Summary\n",
    "* In this project, I implemented two methods to read data from S3 by Spark and Redshift. After loading data from S3 using Spark, I did data quality check and data cleaning using Spark DF and Spark SQL. Then uploaded table to S3 in parquet format.\n",
    "* Amazon S3 is selected as the data lake tool to store the raw csv and parquet staging data before the data is uploaded to the Amazon Redshift data warehouse. \n",
    "* Parquet is selected as the data format for the staging data in S3 because it is in columnar storage and minimizes latency, thus allowing a more efficient data retrieval and processing.\n",
    "* Apache Spark as a distributed data processing framework allows us to efficiently load and transform huge datasets from the raw datasource to the S3 data lake and load to the Redshift data warehouse.\n",
    "* I also created fact and dimension tables after reading parquet format data from S3 into redshift. When reading parquet data, the data type must match between parquet data and tables to be inserted.\n",
    "* The data should be updated based on the MovieLens datasets.\n",
    "* How I would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x. \n",
    "   - Writing data by partitions to S3 and distributing data to different nodes in redshift by distkey and sortkey. Writing data by partitions in s3 can improve the speed a lot. Redshift is a cloud data warehouse that is optimized for aggregation and read-heavy workloads.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "   - Using Airflow to do the management. Creating Airflow allowed us to programmatically schedule our workflows and monitor them via the built-in Airflow user interface.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "   - Amazon Redshift, in which this data model is hosted, allows up to 500 concurrent users accessing the database.\n",
    "   - Users can connect to the data model with Amazon QuickSight to create dashboards and analyze the dataset.\n",
    "   - We can also manage user access and permission with the AWS IAM, so that we can control which users can access which dashboards and the underlying dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
