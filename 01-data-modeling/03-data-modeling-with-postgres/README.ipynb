{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Modeling for Sparkify (Music-Streaming Startup)\n",
    "\n",
    "![SparkifyLogo](img/SparkifyLogo.PNG)\n",
    "\n",
    "## Introduction\n",
    "A music-streaming startup called Sparkify wants to effectively analyze their exponentially-growing data related to songs and user activities on its platform. The Sparkify management team has assigned the task of analyzing the data to the analytics team, in which I belong to. \n",
    "\n",
    "To begin this journey, we decided to build a scalable data warehouse system, which integrates various datasets, such as songs and user-activities, from the Sparkify app.\n",
    "\n",
    "![SparkifyDataWarehouse](img/SparkifyDataWarehouse.PNG)\n",
    "\n",
    "## Project Datasets\n",
    "* Song data: 'data/song_data'  \n",
    "* Log data: 'data/log_data' \n",
    "\n",
    "#### Song Dataset\n",
    "The first dataset contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.  \n",
    "* song_data/A/B/C/TRABCEI128F424C983.json  \n",
    "* song_data/A/A/B/TRAABJL12903CDCF1A.json\n",
    "\n",
    "And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.  \n",
    "> {\"num_songs\": 1, \"artist_id\": \"ARJIE2Y1187B994AB7\", \"artist_latitude\": null, \"artist_longitude\": null, \"artist_location\": \"\", \"artist_name\": \"Line Renaud\", \"song_id\": \"SOUPIRU12A6D4FA1E1\", \"title\": \"Der Kleine Dompfaff\", \"duration\": 152.92036, \"year\": 0}\n",
    "\n",
    "#### User-Activity Log Dataset\n",
    "The log files in the dataset we'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.  \n",
    "* log_data/2018/11/2018-11-12-events.json   \n",
    "* log_data/2018/11/2018-11-13-events.json     \n",
    "\n",
    "And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.  \n",
    "> {\"artist\":null,\"auth\":\"Logged In\",\"firstName\":\"Walter\",\"gender\":\"M\",\"itemInSession\":0,\"lastName\":\"Frye\",\"length\":null,\"level\":\"free\",\"location\":\"San Francisco-Oakland-Hayward, CA\",\"method\":\"GET\",\"page\":\"Home\",\"registration\":1540919166796.0,\"sessionId\":38,\"song\":null,\"status\":200,\"ts\":1541105830796,\"userAgent\":\"\\\"Mozilla\\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\\/537.36 (KHTML, like Gecko) Chrome\\/36.0.1985.143 Safari\\/537.36\\\"\",\"userId\":\"39\"}\n",
    "\n",
    "\n",
    "## Schema for Song Play Analysis\n",
    "Using the song and log datasets(json), we'll create a star schema optimized for queries on song play analysis. This includes the following tables.\n",
    "\n",
    "### Fact Table\n",
    "1.**songplays** - records in log data associated with song plays i.e. records with page NextSong (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "### Dimension Tables\n",
    "2.**users** - users in the app (user_id, first_name, last_name, gender, level)   \n",
    "3.**songs** - songs in music database (song_id, title, artist_id, year, duration)  \n",
    "4.**artists** - artists in music database (artist_id, name, location, latitude, longitude)  \n",
    "5.**time** - timestamps of records in songplays broken down into specific units (start_time, hour, day, week, month, year, weekday)\n",
    "\n",
    "\n",
    "## Project Instructions and Specifications\n",
    "### Create Tables\n",
    "* First step: run create_tables.py to create and connect to the Sparkify database, drops any tables if they exist, and creates the tables\n",
    "* Run test.ipynb to confirm the creation of tables with the correct columns. Make sure to click \"Restart kernel\" to close the connection to the database after running this notebook.\n",
    "\n",
    "### Build ETL Processes\n",
    "* etl.ipynb reads and processes a single file from song_data and log_data and loads the data into the relevant tables. This notebook contains detailed instructions on the ETL process for each of the tables.\n",
    "* Follow instructions and run each line in the etl.ipynb notebook to process the data for each table. \n",
    "* run test.ipynb to confirm that records were successfully inserted into each table. Remember to rerun create_tables.py to reset wer tables before each time we run this notebook.\n",
    "\n",
    "### Build ETL Pipeline\n",
    "* The script etl.py is derived from  the etl.ipynb above. It connects to the Sparkify database, extracts and processes the log_data and song_data, and loads data into the five tables.\n",
    "* Follow instructions in the etl.py notebook to run the ETL processes for each table. \n",
    "* run test.ipynb to confirm that records were successfully inserted into each table. Remember to rerun create_tables.py to reset wer tables before each time we run this notebook.\n",
    "\n",
    "### Final\n",
    "* Run etl in console, and verify results: Python3 create_tables.py && python3 etl.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
